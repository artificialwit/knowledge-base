# AI and LLM related Glossay

## Crunching
Processing of large datasets and complex computations needed to train, fine-tune, or infer results using machine learning models. Here are some key examples:

#### Data Crunching
Cleaning, processing, and analyzing massive datasets before feeding them into an AI model.
  - Example: Parsing millions of records to extract patterns for an AI-driven analytics system.
#### Model Training Crunching
Performing high-intensity calculations on GPUs/TPUs to train deep learning models.
  - Example: Training an LLM like GPT or a vision model requires crunching terabytes of data.
#### Inference Crunching
Running computations on a trained AI model to generate predictions in real-time.
- Example: A chatbot analyzing user queries and generating intelligent responses.
#### Decision-Making Crunching
AI agents evaluating multiple factors before making decisions.
- Example: Your AI agent processing workflows, API calls, and business rules to determine the next action.

## Vertical AI vs AI Agents
### Vertical AI 
refers to AI systems designed for specific industries or domains, such as healthcare, finance, or manufacturing. These AI models are highly specialized, trained on domain-specific data, and optimized for tasks like medical diagnosis, fraud detection, or predictive maintenance. Vertical AI is typically built with a deep understanding of industry challenges, regulations, and workflows, making it highly effective but less flexible outside its intended domain.

### AI Agents
on the other hand, are autonomous systems that can interact with environments, make decisions, and perform tasks based on user inputs, rules, or learned behavior. Unlike Vertical AI, which is specialized, AI agents can be more dynamic, adapting to different contexts by integrating various models and logic. These agents can automate workflows, call APIs, process natural language, and execute actions across multiple domains, making them versatile for business automation, personal assistance, and task management.

## RAG
Retrieval-augmented generation (RAG) is an AI framework that enhances large language models (LLMs) by retrieving information from external sources. This improves the quality of the output and the user experience. 
